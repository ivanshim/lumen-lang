# AI Language Schema (YAML)
# An ideal language for machine learning and AI workflows
# Design synthesized from Python (readability), Rust (safety), and Lumen (semantic clarity)

name: ai
version: 0.1
purpose: "Clear, composable, verifiable AI and machine learning workflows"

# LANGUAGE PHILOSOPHY
# ===================

design_principles:
  1_semantic_clarity: "Every operation's data flow must be visible in syntax"
  2_no_implicit_broadcasting: "Type mismatches fail early with clear error messages"
  3_immutable_by_default: "Values are immutable unless explicitly marked mutable"
  4_explicit_function_types: "Function signatures show inputs, outputs, and transformations"
  5_tensor_as_first_class: "Tensors/arrays are native types with clear semantics"
  6_pattern_matching_first: "Control flow uses pattern matching for ML workflows"
  7_fail_fast_fail_clearly: "Type errors caught at parse time, runtime errors descriptive"
  8_composable_pipelines: "Functions compose naturally without wrapper overhead"
  9_transparent_computation: "All operations are traceable and explainable"
  10_minimal_syntax: "Every syntactic element serves semantic purpose"

rationale: |
  AI systems must be trustworthy and verifiable. This language prioritizes:
  - Making data transformations explicit (no hidden dimensions, broadcasting, or coercion)
  - Type safety without verbosity (inferred where possible, annotated where needed)
  - Composable operations that chain naturally
  - Clear error messages that guide debugging
  - Semantics that survive re-implementation (portable across frameworks)

# LEXICAL RULES
# ==============

keywords:
  # Type definition
  - type
  - struct
  - enum

  # Control flow
  - if
  - else
  - match
  - case

  # Function definition
  - fn
  - return

  # Loops
  - for
  - while
  - break
  - continue

  # Scope
  - let
  - mut

  # Error handling
  - try
  - catch
  - throw

  # Special values
  - true
  - false
  - null
  - none

  # Visibility
  - pub
  - private

  # Type markers
  - as
  - in

# Multi-character operators (maximal munch)
multichar_lexemes:
  - "=>"
  - "->"
  - "|>"
  - "=="
  - "!="
  - "<="
  - ">="
  - "&&"
  - "||"
  - "**"
  - "::"
  - ".."

# Literals
literals:
  number:
    description: "Integer or float literal"
    examples: ["42", "3.14", "1e-5", "0xFF"]
    type_inference: "Inferred as i32, f64, or specific type from context"

  string:
    description: "UTF-8 string literal"
    syntax: "\"hello\" or 'hello'"
    examples: ["\"text\"", "'single'", "\"multi\\nline\""]

  tensor:
    description: "Tensor/array literal with explicit shape"
    syntax: "[1, 2, 3] or [[1, 2], [3, 4]] or Tensor(shape=[N, M], values=[...])"
    type: "Tensor<T, Shape>"
    immutable: "Tensors are immutable by default"

  boolean:
    values: ["true", "false"]

  none:
    value: "none"
    meaning: "No value (similar to null, but type-safe via Option type)"

# Comments
comments:
  line: "// comment to end of line"
  block: "/* ... */ (may nest)"
  docstring: "/// for function/type documentation"

# STRUCTURAL RULES
# ================

structure:
  indentation: "None significant; braces delimit blocks"
  newline_behavior: "Ignored (semicolon terminates statements)"
  block_delimiters:
    open: "{"
    close: "}"
  statement_terminator: ";"
  pipe_operator: "|> (data pipeline, left-to-right composition)"

# CORE TYPE SYSTEM
# ================

types:

  primitive_types:
    number:
      types:
        - "i32: 32-bit signed integer"
        - "i64: 64-bit signed integer"
        - "f32: 32-bit IEEE 754 float"
        - "f64: 64-bit IEEE 754 float"
      coercion: "No implicit coercion; use 'as' for explicit cast"
      example: "x: f64 = 3.14"

    boolean:
      type: "bool"
      values: ["true", "false"]
      truthiness: "Only bool is truthy; numbers/strings don't coerce to bool"

    string:
      type: "String (owned) or &str (borrowed)"
      immutable: "All strings are immutable"
      operations: "Concatenation via +, length, slicing, equality"
      example: "message: String = \"hello\""

    none:
      type: "None (unit type)"
      use: "Return type when nothing is returned"
      meaning: "Type-safe null (used in Option type)"

  composite_types:

    tensor:
      description: "Multi-dimensional array (first-class type)"
      type: "Tensor<T, Shape> where T is element type, Shape is dimensions"
      syntax: "[1, 2, 3] or Tensor(shape=[3], values=[1, 2, 3])"
      immutable: "Default immutable; 'mut x: Tensor' for mutable"
      operations:
        - "Indexing: t[0] or t[0][1]"
        - "Slicing: t[1:5] or t[::2]"
        - "Shape: t.shape() -> [N, M]"
        - "Reshape: t.reshape([N*M])"
        - "Element-wise operations: t1 + t2, t1 * t2"
      broadcasting: "Explicit only - use reshape() or broadcast() for shape alignment"
      type_checking: "Shape validation at compile time where possible"
      example: "data: Tensor<f64, [N, 784]> = load_mnist()"

    array:
      description: "Fixed-size homogeneous collection"
      type: "Array<T, N> (size known at compile time)"
      syntax: "[1, 2, 3] or Array(T, 5)"
      immutable: "Immutable by default"

    tuple:
      description: "Fixed-size heterogeneous collection"
      type: "Tuple<T1, T2, ...>"
      syntax: "(x: i32, y: f64, name: String)"
      destructuring: "let (x, y) = point"
      example: "coordinates: (f64, f64, f64) = (1.0, 2.0, 3.0)"

    option:
      description: "Explicit optional type (no null pointer dereference)"
      type: "Option<T> = Some(T) | None"
      operations: "match, unwrap(), unwrap_or(default), map(f)"
      required: "Must be used instead of null - no implicit null"
      example: "maybe_value: Option<f64> = try_compute()"

    result:
      description: "Explicit error handling type"
      type: "Result<T, E> = Ok(T) | Err(E)"
      operations: "match, unwrap(), unwrap_or(default), map(), catch()"
      required: "All fallible operations return Result; errors are explicit"
      example: "value: Result<Tensor, ParseError> = parse_data(input)"

    function_type:
      description: "First-class function type"
      syntax: "fn(input: T) -> output: U"
      example: "relu: fn(Tensor<f64>) -> Tensor<f64>"
      higher_order: "Functions can take/return functions"
      partial_application: "Supported via lambda"

    custom_struct:
      description: "Named composite type"
      syntax: "struct Point { x: f64, y: f64 }"
      fields: "Immutable by default; 'mut field' for mutable"
      methods: "Defined via impl blocks"

    custom_enum:
      description: "Tagged union type"
      syntax: "enum Color { Red, Green, Blue } or enum Option { Some(T), None }"
      pattern_matching: "Primary control flow mechanism"

# OPERATORS
# =========

operators:

  arithmetic:
    "+": "Addition (numbers and string concatenation)"
    "-": "Subtraction"
    "*": "Multiplication (and tensor operations)"
    "/": "Division"
    "%": "Modulo"
    "**": "Exponentiation"

  comparison:
    "==": "Equality (must have same type)"
    "!=": "Inequality"
    "<": "Less than"
    "<=": "Less than or equal"
    ">": "Greater than"
    ">=": "Greater than or equal"

  logical:
    "&&": "Logical AND (short-circuit)"
    "||": "Logical OR (short-circuit)"
    "!": "Logical NOT"

  tensor_operations:
    "+": "Element-wise addition (shapes must match)"
    "-": "Element-wise subtraction (shapes must match)"
    "*": "Element-wise multiplication (Hadamard product)"
    "/": "Element-wise division"
    "**": "Element-wise exponentiation"
    "@": "Matrix multiplication (T1[M, K] @ T2[K, N] = Result[M, N])"

  type_operations:
    "as": "Explicit type cast (e.g., x as f64)"
    "->": "Function return type annotation"
    "::": "Module/namespace separator"
    "|>": "Pipe operator (pass result to next function)"

  assignment:
    "=": "Assignment (both let and update)"
    "+=": "Add-assign"
    "-=": "Subtract-assign"
    "*=": "Multiply-assign"
    "/=": "Divide-assign"

  special:
    "=>": "Match arm syntax (pattern => expression)"
    "..": "Range operator (1..10)"
    "|": "Pattern or (match a | b => ...)"

# OPERATOR PRECEDENCE (High to Low)
# ==================================

precedence:
  level_1:
    name: "Postfix operations"
    operators: ["[index]", ".method()", ".field", "(call)"]

  level_2:
    name: "Exponentiation"
    operators: ["**"]
    associativity: "right"

  level_3:
    name: "Unary"
    operators: ["-x", "!x"]

  level_4:
    name: "Multiplicative"
    operators: ["*", "/", "%", "@"]
    associativity: "left"

  level_5:
    name: "Additive"
    operators: ["+", "-"]
    associativity: "left"

  level_6:
    name: "Range"
    operators: [".."]

  level_7:
    name: "Relational"
    operators: ["<", "<=", ">", ">="]

  level_8:
    name: "Equality"
    operators: ["==", "!="]

  level_9:
    name: "Logical AND"
    operators: ["&&"]

  level_10:
    name: "Logical OR"
    operators: ["||"]

  level_11:
    name: "Assignment"
    operators: ["=", "+=", "-=", "*=", "/="]
    associativity: "right"

  level_12:
    name: "Pipe"
    operators: ["|>"]
    associativity: "left"
    description: "Data flows left to right through pipeline"

# STATEMENTS
# ==========

statements:

  variable_binding:
    description: "Bind value to name"
    syntax: "let name: Type = value; or let name = value;"
    mutability: "let name is immutable; let mut name is mutable"
    shadowing: "New let in same scope creates new binding"
    example: |
      let x: f64 = 3.14;
      let mut counter: i32 = 0;
      let data: Tensor<f64, [N, M]> = load_data();

  assignment:
    description: "Update mutable variable"
    syntax: "name = value;"
    requirements: "Variable must be declared with 'mut'"
    type_check: "New value must match declared type"
    example: "counter = counter + 1;"

  function_definition:
    description: "Define named function"
    syntax: "fn name(param1: Type1, param2: Type2) -> ReturnType { ... }"
    immutable_params: "Parameters immutable by default"
    return: "Last expression (no semicolon) is return value"
    explicit_return: "return value; for early return"
    example: |
      fn add(x: f64, y: f64) -> f64 {
          x + y
      }

  if_statement:
    description: "Conditional execution"
    syntax: "if condition { ... } else if condition { ... } else { ... }"
    pattern_matching: "if let pattern = value { ... }"
    example: |
      if prediction > threshold {
          classify_as_positive();
      } else {
          classify_as_negative();
      }

  match_statement:
    description: "Pattern matching (preferred over if for multiple cases)"
    syntax: "match value { pattern => expr, ... }"
    exhaustiveness: "Must cover all cases (compiler checks)"
    example: |
      match result {
          Ok(value) => process(value),
          Err(error) => log_error(error),
      }

  for_loop:
    description: "Iterate over collection"
    syntax: "for item in collection { ... } or for i in 0..10 { ... }"
    destructuring: "for (x, y) in points { ... }"
    example: |
      for batch in data_loader {
          loss = train_step(batch, model);
      }

  while_loop:
    description: "Conditional loop"
    syntax: "while condition { ... }"
    example: |
      while loss > convergence_threshold {
          update_weights();
          loss = compute_loss();
      }

  try_catch:
    description: "Error handling"
    syntax: "try { ... } catch(error) { ... }"
    result_based: "Functions return Result<T, E> by default"
    unwrapping: "Use 'try!' macro to propagate errors"
    example: |
      try {
          data = load_file("model.pth");
      } catch(error) {
          log("Failed to load: " + error.message);
      }

  return_statement:
    description: "Early return from function"
    syntax: "return value;"
    implicit: "Last expression is implicit return"

  break_statement:
    description: "Exit loop"
    syntax: "break;"

  continue_statement:
    description: "Skip to next loop iteration"
    syntax: "continue;"

# EXPRESSIONS
# ===========

expressions:

  literals:
    numbers: "42, 3.14, 1e-5"
    strings: "\"hello\""
    tensors: "[1, 2, 3] or [[1, 2], [3, 4]]"
    booleans: "true, false"

  function_call:
    syntax: "function(arg1, arg2)"
    partial_application: "func(x) creates new function with x bound"

  method_call:
    syntax: "object.method(args)"
    auto_deref: "Automatic dereferencing for convenience"

  pipe_operator:
    syntax: "value |> function1() |> function2() |> function3()"
    meaning: "Left-to-right data flow, more readable than nested calls"
    example: |
      data
          |> normalize()
          |> augment()
          |> batch()
          |> train_step()

  lambda:
    syntax: "lambda(x, y) => x + y"
    example: "relu = lambda(x) => max(0, x)"
    composition: "Lambdas compose naturally with pipe operator"

  pattern_match:
    syntax: "match expr { pattern => expr, ... }"
    destructuring: "match point { (0, 0) => \"origin\", (x, y) => ... }"

  tensor_operations:
    element_wise: "t1 + t2, t1 * t2, t1 / t2"
    matrix_multiply: "t1 @ t2 (shapes must align for matrix mult)"
    slicing: "t[1:5], t[:, 0], t[::2]"
    indexing: "t[0], t[i][j]"
    reshape: "t.reshape([new_shape])"
    transpose: "t.transpose()"

# KEY LANGUAGE CONSTRUCTS FOR AI
# ===============================

ai_specific_features:

  tensor_type:
    description: "First-class immutable tensor type"
    syntax: "Tensor<T, Shape>"
    operations: "Shape-aware indexing, slicing, broadcasting (explicit)"
    type_safety: "Shape checking at compile time where possible"
    immutability: "Tensors immutable by default (functional style)"
    efficiency: "Compiler can optimize without mutable-state tracking"

  explicit_broadcasting:
    description: "No implicit broadcasting - must be explicit"
    why: "Prevents silent dimension errors in ML code"
    example: |
      // This FAILS at compile time:
      // result = tensor[N, M] + vector[M]  // Error: shape mismatch

      // Correct approach:
      // result = tensor[N, M] + vector.reshape([1, M]).broadcast([N, M])

  pipeline_operator:
    description: "Data flows left-to-right through transformations"
    benefit: "Reads naturally for ML workflows (data -> transform -> model -> loss)"
    example: |
      accuracy = dataset
          |> load()
          |> normalize()
          |> split_test()
          |> apply_model()
          |> compute_accuracy()

  pattern_matching:
    description: "Primary control flow for ML decisions"
    benefit: "Exhaustiveness checking ensures all cases handled"
    example: |
      match model_output {
          Classification(probs) => apply_softmax(probs),
          Regression(value) => postprocess(value),
          Error(msg) => handle_error(msg),
      }

  result_type:
    description: "Explicit error handling"
    benefit: "All operations that can fail return Result; no surprises"
    example: |
      result = try_load_model("weights.pth");
      model = match result {
          Ok(m) => m,
          Err(e) => { log("Using pretrained"); get_pretrained() },
      }

  function_composition:
    description: "Functions as first-class values"
    benefit: "Build complex pipelines from simple functions"
    example: |
      relu = lambda(x) => max(0, x);
      sigmoid = lambda(x) => 1 / (1 + exp(-x));
      activation = compose(relu, sigmoid);

  immutable_by_default:
    description: "Values immutable unless declared mutable"
    benefit: "Prevents accidental mutations, easier to reason about"
    example: |
      let weights: Tensor = init_weights();
      // weights[0] = 0.5;  // ERROR: immutable

      let mut learning_rate: f64 = 0.001;
      learning_rate = 0.0001;  // OK: declared mutable

  type_annotations:
    description: "Explicit where needed, inferred where possible"
    benefit: "Code is self-documenting without verbosity"
    example: |
      fn transform(input: Tensor<f64, [N, D]>) -> Tensor<f64, [N, D]> {
          // Shape is explicit - compiler validates transformations
          output = input |> dense(hidden_size) |> relu() |> dense(D);
          output
      }

# LIBRARY ECOSYSTEM (SUGGESTED)
# =============================

stdlib:
  tensor_ops: "reshape, transpose, slice, broadcast, concatenate, stack"
  math: "sin, cos, exp, log, sqrt, max, min, mean, sum, std"
  linear_algebra: "matmul, inv, solve, eigh, svd"
  random: "normal, uniform, shuffle, choice"
  io: "load, save (for tensors, models, data)"
  functional: "map, filter, reduce, compose, pipe"
  statistics: "mean, variance, percentile, histogram"
  loss_functions: "mse, crossentropy, mae, huber"
  optimizers: "sgd, adam, rmsprop, momentum"
  metrics: "accuracy, precision, recall, f1"

# DESIGN GUARANTEES
# =================

guarantees:
  type_safety: "All type mismatches caught at compile time"
  no_implicit_coercion: "No silent type conversions; 'as' keyword required"
  no_broadcast_surprises: "All shape mismatches caught early; explicit broadcasting required"
  no_null_dereference: "Option type prevents null pointer errors"
  no_unhandled_errors: "Result type requires error handling; compiler enforces"
  determinism: "Same input always produces same output (no hidden state)"
  explicitness: "All operations visible in syntax; no magic"
  verifiability: "Code structure matches execution flow (AST is truth)"

# EXAMPLE: TRAINING LOOP
# ======================

example_training_loop: |
  // Train a neural network with clear data flow

  fn train_step(model: &mut Model, batch: Tensor<f64, [B, D]>) -> f64 {
      // Forward pass
      logits: Tensor = model.forward(batch);

      // Compute loss
      loss_result = compute_loss(logits, batch.labels);
      loss: f64 = match loss_result {
          Ok(l) => l,
          Err(e) => { print("Loss error: " + e); return 0.0; }
      };

      // Backward pass (gradient computation)
      grads = compute_gradients(model, loss);

      // Update weights
      model = apply_gradients(model, grads, learning_rate);

      loss  // Return loss value
  }

  // Main training loop
  fn train(
      train_data: Tensor<f64, [N, D]>,
      model: &mut Model,
      epochs: i32
  ) -> Model {
      for epoch in 0..epochs {
          total_loss: f64 = 0.0;
          batch_count: i32 = 0;

          for batch in make_batches(train_data, batch_size) {
              loss = train_step(model, batch);
              total_loss = total_loss + loss;
              batch_count = batch_count + 1;
          }

          avg_loss = total_loss / batch_count as f64;
          print("Epoch " + epoch + " loss: " + avg_loss);
      }

      model
  }

# PORTABILITY OF THOUGHT
# ======================

portability: |
  This language specification is implementation-independent:

  - No reference to specific frameworks (TensorFlow, PyTorch, JAX, etc.)
  - No reference to specific hardware (GPU, TPU, CPU optimizations)
  - No reference to specific languages (transpilable to multiple targets)
  - Semantics survive re-implementation: anyone reading this spec
    can implement it in any language and get identical behavior

  The principles are about thought, not execution:
  - Clear data flow (visible in code, not hidden in framework)
  - Explicit type handling (no surprise coercions)
  - Composable operations (functions combine naturally)
  - Verifiable results (code structure matches computation structure)

# GROWTH DISCIPLINE
# ==================

constraints:
  simplicity: "Language must remain explainable in one sitting"
  inspectability: "Full implementation must be inspectable by one person"
  no_feature_creep: "New features only when real examples demand them"
  preservation: "New features cannot invalidate existing mental models"
  minimalism: "If removed feature isn't missed, remove it"

# FINAL DESIGN TEST
# ==================

design_test: |
  A change is acceptable only if it makes programs:

  ✓ Easier to reason about
  ✓ Easier to read later
  ✓ Harder to misinterpret

  NOT shorter.
  NOT faster.
  CLEARER.

  This language is designed for trustworthy, verifiable AI systems
  where clarity and correctness matter more than cleverness.
